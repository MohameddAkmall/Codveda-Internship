{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMe3hp0fUFd5dJGMdQ2b0BF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohameddAkmall/Codveda-Internship/blob/main/Codveda_House_Predection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and Inspect Data"
      ],
      "metadata": {
        "id": "dvW-skQFujdM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX9fqXXtrukA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Column names from Boston Housing dataset\n",
        "columns = [\n",
        "    \"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\",\n",
        "    \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"\n",
        "]\n",
        "\n",
        "# Load dataset (assuming CSV format)\n",
        "df = pd.read_csv(\"/content/4) house Prediction Data Set.csv\", delim_whitespace=True, header=None, names=columns)\n",
        "\n",
        "# Inspect\n",
        "print(df.head())\n",
        "print(df.info())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handle Missing Data"
      ],
      "metadata": {
        "id": "z9qEgq0yumq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check missing values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Option 1: Fill with mean\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "# Option 2: Drop missing rows\n",
        "# df.dropna(inplace=True)\n"
      ],
      "metadata": {
        "id": "y3oN0h3dt8tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode Categorical Variables\n",
        "\n",
        "CHAS is categorical (0 = not near river, 1 = near river).\n",
        "Since it’s binary, no need for one-hot encoding"
      ],
      "metadata": {
        "id": "YD2GDMCuuuIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df, drop_first=True)\n"
      ],
      "metadata": {
        "id": "HlklBJvKuFgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize or Standardize Numerical Features\n",
        "\n",
        "Standardization: mean = 0, std = 1\n",
        "\n",
        "Normalization: scale between 0–1"
      ],
      "metadata": {
        "id": "2C07_zM0u44k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = df.drop(\"MEDV\", axis=1)\n",
        "y = df[\"MEDV\"]\n",
        "\n",
        "X_scaled = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "CggALC0muKMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split Dataset into Training and Testing Sets"
      ],
      "metadata": {
        "id": "uajjsnJovBuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "137_GcI2uOrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "linear regression model"
      ],
      "metadata": {
        "id": "DFsJVLZrvq6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Linear Regression Model"
      ],
      "metadata": {
        "id": "W2BUig5KvvfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "_PRF7VD9vGf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpret Model Coefficients\n",
        "\n",
        "Each coefficient shows the effect of that feature on house price.\n",
        "\n",
        "Example: if RM coefficient = 3, then each additional room increases price by ~3 units (keeping other features constant)."
      ],
      "metadata": {
        "id": "qXH-EptdwCes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coefficients = pd.DataFrame({\n",
        "    \"Feature\": X.columns,\n",
        "    \"Coefficient\": model.coef_\n",
        "})\n",
        "print(coefficients)\n"
      ],
      "metadata": {
        "id": "AGtGf9z6v3KW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Model Performance"
      ],
      "metadata": {
        "id": "v47_XRNBwRI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n"
      ],
      "metadata": {
        "id": "Toqqhb0uwHb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Predictions vs Actual Values"
      ],
      "metadata": {
        "id": "-lgwgbRwyRwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scatter plot of actual vs predicted\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(y_test, y_pred, color=\"blue\", alpha=0.6)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
        "         color=\"red\", linewidth=2)  # perfect prediction line\n",
        "plt.xlabel(\"Actual MEDV\")\n",
        "plt.ylabel(\"Predicted MEDV\")\n",
        "plt.title(\"Actual vs Predicted House Prices\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mrsvWO5syOdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree Classifier (categorize prices)\n",
        "\n",
        "the house price (MEDV) — is continuous (e.g., 24.0, 21.6, 33.4...).\n",
        "But DecisionTreeClassifier is for classification, meaning it expects discrete labels (like 0, 1, 2 for classes).\n",
        "\n",
        "Thats why we will use Decision Tree Regressor."
      ],
      "metadata": {
        "id": "jIIEHDOgzwmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Train a Decision Tree Regressor (unpruned)\n",
        "tree_reg = DecisionTreeRegressor(random_state=42)\n",
        "tree_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = tree_reg.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"R² Score:\", r2_score(y_test, y_pred))\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "F7_qp1fuzxoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "prune the tree"
      ],
      "metadata": {
        "id": "8kBTdzI30m9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pruned tree using manual constraints\n",
        "pruned_tree = DecisionTreeRegressor(\n",
        "    random_state=42,\n",
        "    max_depth=3,             # Limit tree depth\n",
        "    min_samples_split=50,    # Minimum samples to split a node\n",
        "    min_samples_leaf=20      # Minimum samples in a leaf\n",
        ")\n",
        "pruned_tree.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_pruned = pruned_tree.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Pruned Tree R² Score:\", r2_score(y_test, y_pred_pruned))\n",
        "print(\"Pruned Tree Mean Squared Error:\", mean_squared_error(y_test, y_pred_pruned))\n"
      ],
      "metadata": {
        "id": "jvTtXTkD0sbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the unpruned Tree"
      ],
      "metadata": {
        "id": "oDLIKb26z1ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "tree.plot_tree(reg_tree, feature_names=X.columns, filled=True, max_depth=3)\n",
        "plt.title(\"Unpruned Decision Tree (Top 3 Levels)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PEDNqtXFz4Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "visualize pruned tree"
      ],
      "metadata": {
        "id": "o9irgkdjiDrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "tree.plot_tree(pruned_tree, feature_names=X.columns, filled=True, max_depth=3)\n",
        "plt.title(\"Pruned Decision Tree (Top 3 Levels)\")\n",
        "plt.show()\n",
        "print(\"Unpruned R²:\", r2_score(y_test, y_pred))\n",
        "print(\"Pruned R²:\", r2_score(y_test, y_pred_pruned))\n",
        "print(\"Unpruned MSE:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"Pruned MSE:\", mean_squared_error(y_test, y_pred_pruned))\n",
        "\n"
      ],
      "metadata": {
        "id": "4NoVd8g3iGOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Metric                                | Unpruned Tree | Pruned Tree | Interpretation                                                                                                                                             |\n",
        "| :------------------------------------ | :------------ | :---------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **R² (coefficient of determination)** | 0.858         | 0.684       | The unpruned tree fits the training data much better — but may overfit. The pruned tree is simpler and sacrifices some accuracy for better generalization. |\n",
        "| **MSE (Mean Squared Error)**          | 10.42         | 23.16       | The pruned tree has higher error — expected, since it’s less complex and less tuned to the training set.                                                   |\n"
      ],
      "metadata": {
        "id": "dTrMEndFi1Gx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "k-mean clustering\n",
        "We’ll cluster house prices into 3 groups (Low, Medium, High) using the Housing dataset."
      ],
      "metadata": {
        "id": "Qoq9O9kY6CVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elbow Method (choose optimal k)"
      ],
      "metadata": {
        "id": "nWf1OX5s6LaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Elbow method to determine optimal number of clusters\n",
        "inertia = []\n",
        "K = range(1, 11)\n",
        "\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_scaled)   # we already scaled X earlier\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(K, inertia, 'bo-')\n",
        "plt.xlabel(\"Number of Clusters (k)\")\n",
        "plt.ylabel(\"Inertia (Within-Cluster Sum of Squares)\")\n",
        "plt.title(\"Elbow Method for Optimal k\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BQxNF3IM6QKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply KMeans with k=3"
      ],
      "metadata": {
        "id": "BvwAKXKE65zH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply KMeans with k=3 (Low, Medium, High groups)\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "clusters = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Add cluster labels to dataframe\n",
        "df[\"Cluster\"] = clusters\n"
      ],
      "metadata": {
        "id": "KArtGtiO7BOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Clusters (2D Scatter)"
      ],
      "metadata": {
        "id": "jYbQmG5v7AuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(df[\"RM\"], df[\"MEDV\"], c=df[\"Cluster\"], cmap=\"viridis\")\n",
        "plt.xlabel(\"Average Rooms per Dwelling (RM)\")\n",
        "plt.ylabel(\"Median Value of Homes (MEDV)\")\n",
        "plt.title(\"KMeans Clustering of House Prices\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G2UChus-7HMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What the graph represents\n",
        "\n",
        "X-axis: Average number of rooms per dwelling (RM)\n",
        "\n",
        "Y-axis: Median value of homes (MEDV)\n",
        "\n",
        "Each dot represents one house (or neighborhood), and the color shows which cluster the K-Means algorithm assigned it to.\n",
        "\n",
        "🟢 Cluster 1 – Low-priced Homes\n",
        "\n",
        "Houses in this group have fewer rooms (typically 4–6 per dwelling).\n",
        "\n",
        "They correspond to lower median home values (around $10,000–$20,000 in the dataset’s scaled values).\n",
        "\n",
        "These are likely older, smaller properties or those located in less desirable neighborhoods.\n",
        "\n",
        "🟡 Cluster 2 – Medium-priced Homes\n",
        "\n",
        "Houses have a moderate number of rooms (around 6–7).\n",
        "\n",
        "The median value of homes in this cluster is mid-range (roughly $20,000–$30,000).\n",
        "\n",
        "These homes represent the average housing market segment, possibly suburban or mid-income neighborhoods.\n",
        "\n",
        "🟣 Cluster 3 – High-priced Homes\n",
        "\n",
        "Houses in this cluster have more rooms (around 7–8+).\n",
        "\n",
        "They correspond to high median home values (around $35,000–$50,000).\n",
        "\n",
        "These are likely large, newer homes in desirable areas with better amenities.\n",
        "\n",
        "The clustering is based on the similarity of data points — K-Means grouped together houses that share similar room counts and median values."
      ],
      "metadata": {
        "id": "BFcVGzcHkdTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------\n",
        "Random forest\n",
        "--------------------------"
      ],
      "metadata": {
        "id": "w3MP0Fssl3vV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Random Forest Regressor\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest model\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=None,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"Random Forest R²: {r2:.4f}\")\n",
        "print(f\"Random Forest MSE: {mse:.4f}\")\n",
        "\n",
        "# Cross-validation (5-fold)\n",
        "cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='r2')\n",
        "print(f\"Cross-Validation R²: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
        "\n",
        "# Feature Importance\n",
        "importances = rf_model.feature_importances_\n",
        "feature_names = X.columns\n",
        "\n",
        "# Visualize feature importance\n",
        "indices = np.argsort(importances)[::-1]\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=importances[indices], y=feature_names[indices], palette=\"viridis\")\n",
        "plt.title(\"Feature Importance (Random Forest Regressor)\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5G-CNK1eklLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "✅ RandomForestRegressor — fits continuous target values like prices.\n",
        "\n",
        "✅ R² (coefficient of determination) — measures how well the model explains variance (closer to 1 = better).\n",
        "\n",
        "✅ MSE (Mean Squared Error) — lower = better fit.\n",
        "\n",
        "✅ Feature importance — shows which input variables most affect predicted prices."
      ],
      "metadata": {
        "id": "9934v-d0nCWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Networks with\n",
        "TensorFlow/Keras\n",
        "-------------------------"
      ],
      "metadata": {
        "id": "HcTitPcEnDxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Neural Networks with TensorFlow/Keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1️⃣ Preprocessing\n",
        "# Scale the features for better NN convergence\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "mm8YSrm5nURI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2️⃣ Build the Neural Network\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # input layer + 1st hidden layer\n",
        "    Dropout(0.2),  # prevents overfitting\n",
        "    Dense(32, activation='relu'),  # 2nd hidden layer\n",
        "    Dense(1)  # output layer for regression\n",
        "])\n",
        "\n",
        "# 3️⃣ Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "# 4️⃣ Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=100,\n",
        "    batch_size=16,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "FMmgAR0pncl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5️⃣ Evaluate performance\n",
        "y_pred = model.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nNeural Network R²: {r2:.4f}\")\n",
        "print(f\"Neural Network MSE: {mse:.4f}\")\n",
        "\n",
        "# 6️⃣ Visualize training vs validation loss\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aEIrFQsTnk2E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}